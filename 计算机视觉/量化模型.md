---
tags:
  - 量化模型
  - 模型压缩
---
- 培训时间：/
- 讲述人：虞若弘
- 培训内容：量化模型介绍
- 培训目标：对模型量化形成初步印象

模型量化是一种将浮点计算转成低比特定点计算的技术，可以有效的**降低模型计算强度、参数大小和内存消耗**，但往往带来巨大的精度损失。尤其是在极低比特(<4bit)、二值网络(1bit)、甚至将梯度进行量化时，带来的**精度挑战更大**。

# 什么是量化

量化是指将信号的连续取值近似为有限多个离散值的过程。可理解成一种信息压缩的方法。在计算机系统上考虑这个概念，一般用“低比特”来表示。也有人称量化为“定点化”，但是严格来讲所表示的范围是缩小的。定点化特指scale为2的幂次的线性量化，是一种更加实用的量化方法。

## 举个栗子

韩松在ICLR2016上获得best paper的论文，首次提出了参数量化方法。
其使用k-mean聚类，让相近的数值聚类到同一个聚类中心，复用同一个数值，从而达到用更少的数值表示更多的数，这是量化操作的一种方案。
我们认为绝大部分的模型量化算法都能压缩参数，因此压缩参数的实用性不存在问题。

目前已知提速概率较大的量化方法
* 二值化
	 * xnor + popcount 理论峰值比 float32 高 
	*  引入额外的 quantizer，可用 SIMD 方式加速
* 线性量化（对称、非对称、Ristretto）
	 * arm/x86/nvGPU均支持高效的8-bit计算，TensorCore支持4bit计算
	 * 引入额外的quantizer/de-quantizer，可用 SIMD方式加速
* 对数量化
	 * 可将乘法转变为加法，加法转变为索引

Tips：SIMD（Single Instruction Multiple Data）单指令流多数据流，简单解释就是，对于一个int(32bit)数组，之前我们处理数据是分别对每一个元素进行处理，但是使用SIMD技术，我们可以同时处理64bit或128bit或256bit或512bit，相当于之前处理一个int的时间我们现在可以处理2个或4个或8个或16个。

# 量化方法加速的条件

1. 量化数值的计算在部署硬件上的峰值性能更高
2. 量化算法引入的额外计算少

理论峰值性能：单位时钟周期内能完成的计算个数  * 芯片频率

# 量化特性带来的好处

模型量化还有一个潜在的好处是降低运行时内存占用，可以得到如下好处：
1. 降低访存量，存在提升速度的可能
2. 在同样硬件环境下，同时处理更多视频或视频路数
3. 训练更大模型
使用方式：将尽可能多的layer的激活值都进行量化 。

# 生产量化模型

借鉴了ICCV2019上一篇data-free量化论文的定义。

L1：直接将一个浮点参数直接转化成量化数，一般会带来很大的精度损失，但使用上非常简单。
L2：基于数据校准的方案，很多芯片都会提供这样的功能，比如tensorRT，高通，寒武纪等。它需要转模型的时候提供一些真实的计算数据。
L3：基于训练finetune的方案，有很多论文都是使用这种方法，它的好处是可以带来更大的精度提升，缺点是需要修改训练代码，实施周期比较长。